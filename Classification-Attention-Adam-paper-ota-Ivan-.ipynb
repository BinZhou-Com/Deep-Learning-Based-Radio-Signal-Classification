{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os,random\n",
    "from tensorflow.keras.layers import Input,Reshape,ZeroPadding2D,Conv2D,Embedding,Dropout,Flatten,Dense,Activation,MaxPooling2D,AlphaDropout\n",
    "from tensorflow.keras.layers import Add, BatchNormalization, Activation, LSTM, Dropout\n",
    "from tensorflow.keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.models as Model\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(r'C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part0.h5')\n",
    "sample_num = f['X'].shape[0]\n",
    "idx = np.random.choice(range(0,sample_num),size=60000)\n",
    "# idx = np.random.choice(range(0,sample_num),size=30000)\n",
    "X = f['X'][:][idx]\n",
    "Y = f['Y'][:][idx]\n",
    "Z = f['Z'][:][idx]\n",
    "f.close()\n",
    "\n",
    "for i in range(1,24):\n",
    "    if i%1 == 0:\n",
    "        !free -m\n",
    "    '''if i == 10:\n",
    "        continue'''\n",
    "    filename = r'C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part'+str(i) + '.h5'\n",
    "    print(filename)\n",
    "    f = h5py.File(filename,'r')\n",
    "    X = np.vstack((X,f['X'][:][idx]))\n",
    "    Y = np.vstack((Y,f['Y'][:][idx]))\n",
    "    Z = np.vstack((Z,f['Z'][:][idx]))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "print('X-size：',X.shape)\n",
    "print('Y-size：',Y.shape)\n",
    "print('Z-size：',Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing, and obtain training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = X.shape[0]\n",
    "n_train = int(n_examples * 0.8)   \n",
    "train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)  #Randomly select training sample subscript\n",
    "test_idx = list(set(range(0,n_examples))-set(train_idx)) #Test sample index\n",
    "X_train = X[train_idx]  #training samples\n",
    "X_test =  X[test_idx]  #testing samples\n",
    "Y_train = Y[train_idx]\n",
    "Y_test = Y[test_idx]\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"Y_train:\",Y_train.shape)\n",
    "print(\"X_test:\",X_test.shape)\n",
    "print(\"Y_test:\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs, name):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "#     TIME_STEPS = inputs.shape[1].value\n",
    "    TIME_STEPS =  inputs.shape.as_list()[1]\n",
    "\n",
    "    SINGLE_ATTENTION_VECTOR = False\n",
    "    \n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name=name)(a)\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shp = X_train.shape[1:]\n",
    "maxlen=X_train.shape[1]\n",
    "classes = ['32PSK',\n",
    " '16APSK',\n",
    " '32QAM',\n",
    " 'FM',\n",
    " 'GMSK',\n",
    " '32APSK',\n",
    " 'OQPSK',\n",
    " '8ASK',\n",
    " 'BPSK',\n",
    " '8PSK',\n",
    " 'AM-SSB-SC',\n",
    " '4ASK',\n",
    " '16PSK',\n",
    " '64APSK',\n",
    " '128QAM',\n",
    " '128APSK',\n",
    " 'AM-DSB-SC',\n",
    " 'AM-SSB-WC',\n",
    " '64QAM',\n",
    " 'QPSK',\n",
    " '256QAM',\n",
    " 'AM-DSB-WC',\n",
    " 'OOK',\n",
    " '16QAM']\n",
    "\n",
    "opt = tfa.optimizers.NovoGrad(\n",
    "    lr=1e-3,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    weight_decay=0.001,\n",
    "    grad_averaging=False\n",
    ")\n",
    "def model1(init):\n",
    "    x = init\n",
    "    x = Conv1D(64, 3,strides=2,padding='same',activation='relu')(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = attention_3d_block(x, 'attention_vec_1')\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    x = attention_3d_block(x, 'attention_vec_2')\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    out = Dense(64, activation=\"relu\")(x)\n",
    "    return out\n",
    "\n",
    "def m2_block(init, filter, kernel, pool):\n",
    "    x = init\n",
    "    \n",
    "    x = Conv1D(filter, kernel, padding='same', kernel_initializer='he_normal', activation='elu')(x)\n",
    "    skip = x\n",
    "    x = Conv1D(filter, kernel, padding='same', kernel_initializer='he_normal', activation='elu')(x)\n",
    "    x = Conv1D(filter, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, skip])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def model2(init):\n",
    "    #init = Reshape((maxlen, embed_size, 1))(init)\n",
    "    \n",
    "    # pool = maxlen - filter + 1\n",
    "    x0 = m2_block(init, 32, 1, maxlen - 1 + 1)\n",
    "    x1 = m2_block(init, 32, 2, maxlen - 2 + 1)\n",
    "    x2 = m2_block(init, 32, 3, maxlen - 3 + 1)\n",
    "    x3 = m2_block(init, 32, 5, maxlen - 5 + 1)\n",
    "    \n",
    "    x = concatenate([x0, x1, x2, x3])\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(64, activation=\"relu\")(x)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(1024,2))\n",
    "    #x = Embedding(max_features, embed_size)(inp)\n",
    "    #x = Embedding(input_dim=max_features, output_dim= embed_size , input_length=maxlen,weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = Reshape([1024,2], input_shape=in_shp)(inp)\n",
    "\n",
    "    out1 = model1(inp)\n",
    "    out2 = model2(inp)\n",
    "    \n",
    "    conc = concatenate([out1, out2])\n",
    "    \n",
    "    #conc = out1\n",
    "    x = Dropout(0.4)(conc)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Reshape((x.shape.as_list()[1], 1))(x)\n",
    "    x = LSTM(len(classes))(x)\n",
    "    outp = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "filepath = 'attention_model.h5'\n",
    "history = model.fit(X_train,\n",
    "    Y_train,\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "#     validation_data=(X_test, Y_test),\n",
    "    validation_split = 0.2,\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2)) = plt.subplots(nrows=1, ncols=2,figsize=(20,6))\n",
    "\n",
    "ax1.plot(history.history['accuracy'],'b', history.history['val_accuracy'], 'r')\n",
    "ax1.set_ylabel('Accuracy Rate',fontsize=12)\n",
    "ax1.set_xlabel('Iteration',fontsize=12)\n",
    "ax1.set_title('Categorical Cross Entropy ',fontsize=14)\n",
    "ax1.legend(['Training Accuracy','Validation Accuracy'],fontsize=12,loc='best')\n",
    "\n",
    "ax2.plot(history.history['loss'], 'b',history.history['val_loss'],'r')\n",
    "ax2.set_ylabel('Loss',fontsize=12)\n",
    "ax2.set_xlabel('Iteration',fontsize=12)\n",
    "ax2.set_title('Learning Curve ',fontsize=14)\n",
    "ax2.legend(['Training Loss','Validation Loss'],fontsize=12,loc='best')\n",
    "\n",
    "# plt.savefig('crosse_results.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "# Plot confusion matrix\n",
    "batch_size = 1024\n",
    "test_Y_hat = model.predict(X_test, batch_size=3000)\n",
    "conf = np.zeros([len(classes),len(classes)])\n",
    "confnorm = np.zeros([len(classes),len(classes)])\n",
    "for i in range(0,X_test.shape[0]):\n",
    "    j = list(Y_test[i,:]).index(1)\n",
    "    k = int(np.argmax(test_Y_hat[i,:]))\n",
    "    conf[j,k] = conf[j,k] + 1\n",
    "for i in range(0,len(classes)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "plot_confusion_matrix(confnorm, labels=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(confnorm)):\n",
    "    print(classes[i],confnorm[i,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "acc={}\n",
    "Z_test = Z[test_idx]\n",
    "Z_test = Z_test.reshape((len(Z_test)))\n",
    "SNRs = np.unique(Z_test)\n",
    "for snr in SNRs:\n",
    "    X_test_snr = X_test[Z_test==snr]\n",
    "    Y_test_snr = Y_test[Z_test==snr]\n",
    "    \n",
    "    pre_Y_test = model.predict(X_test_snr)\n",
    "    conf = np.zeros([len(classes),len(classes)])\n",
    "    confnorm = np.zeros([len(classes),len(classes)])\n",
    "    for i in range(0,X_test_snr.shape[0]):    #该信噪比下测试数据量\n",
    "        j = list(Y_test_snr[i,:]).index(1)   #正确类别下标\n",
    "        k = int(np.argmax(pre_Y_test[i,:])) #预测类别下标\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(classes)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "   \n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confnorm, labels=classes, title=\"ConvNet Confusion Matrix (SNR=%d)\"%(snr))\n",
    "    \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    print (\"Overall Accuracy %s: \"%snr, cor / (cor+ncor))\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(acc.keys(),acc.values())\n",
    "plt.ylabel('ACC')\n",
    "plt.xlabel('SNR')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
