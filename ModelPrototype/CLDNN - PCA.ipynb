{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'snr_val = -20   # SNR Value to train using'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Modulation Recognition Example: RML2016.10b Dataset + VT-CNN2 Mod-Rec Network\n",
    "# More information on this classification method can be found at\n",
    "# https://arxiv.org/abs/1602.04105\n",
    "# More information on the RML2016.10b dataset can be found at\n",
    "# http://pubs.gnuradio.org/index.php/grcon/article/view/11\n",
    "# Please cite derivative works\n",
    "# ```\n",
    "# @article{convnetmodrec,\n",
    "#   title={Convolutional Radio Modulation Recognition Networks},\n",
    "#   author={O'Shea, Timothy J and Corgan, Johnathan and Clancy, T. Charles},\n",
    "#   journal={arXiv preprint arXiv:1602.04105},\n",
    "#   year={2016}\n",
    "# }\n",
    "# @article{rml_datasets,\n",
    "#   title={Radio Machine Learning Dataset Generation with GNU Radio},\n",
    "#   author={O'Shea, Timothy J and West, Nathan},\n",
    "#   journal={Proceedings of the 6th GNU Radio Conference},\n",
    "#   year={2016}\n",
    "# }\n",
    "# ```\n",
    "# To run this example, you will need to download or generate the RML2016.10b dataset (https://radioml.com/datasets/)\n",
    "# You will also need Keras installed with either the Theano or Tensor Flow backend working.\n",
    "# Have fun!\n",
    "\n",
    "\"\"\"\n",
    "Run code for CLDNN Sub-Sampling, PCA and SNR Training experiments by uncommenting the appropriate code blocks\n",
    "For all the experiments, the value on line 224 (11200 for no modifications) must be changed according to the dimensions specified-\n",
    "128 (1) -> 11200; 64 (1/2) -> 6080; 32 (1/4) -> 3520; 16 (1/8) -> 2240; 8 (1/16) -> 1600; 4 (1/32) -> 1280\n",
    "For PCA experiments: Uncomment 'PCA Setup' and 'PCA' code blocks\n",
    "For Sub-Sampling experiments: Uncomment 'Sub-Sampling Setup' and 1 of the 3 subsampling code blocks\n",
    "For Individual SNR Training experiments: Uncomment 'SNR Setup' and 'SNR Training' code blocks\n",
    "For no dimensionality reduction experiments: Run the code as is without uncommenting any code block\n",
    "\"\"\"\n",
    "\n",
    "# In[1]:\n",
    "# Import all the things we need ---\n",
    "# by setting env variables before Keras import you can set up which backend and which GPU it uses\n",
    "#get_ipython().magic(u'matplotlib inline')\n",
    "import os,random\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "#os.environ[\"THEANO_FLAGS\"]  = \"device=gpu%d\"%(1)   #disabled because we do not have a hardware GPU\n",
    "import numpy as np\n",
    "#import theano as th\n",
    "#import theano.tensor as T\n",
    "from keras.utils import np_utils\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, LSTM\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random, sys, keras\n",
    "from keras.utils import multi_gpu_model\n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "import h5py\n",
    "\n",
    "\n",
    "def _get_available_gpus():\n",
    "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
    "\n",
    "    # Returns\n",
    "        A list of available GPU devices.\n",
    "    \"\"\"\n",
    "    #global _LOCAL_DEVICES\n",
    "    if tfback._LOCAL_DEVICES is None:\n",
    "        devices = tf.config.list_logical_devices()\n",
    "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
    "\n",
    "tfback._get_available_gpus = _get_available_gpus\n",
    "import tensorflow as tf\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Sub-Sampling Setup\n",
    "sub_samples = 16   # Number of samples after Sub-Sampling\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# PCA Setup\n",
    "from sklearn.decomposition import PCA\n",
    "pca_rate=2   # Number of samples after PCA\n",
    "pca = PCA(n_components=pca_rate*2)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# SNR Setup\n",
    "\"\"\"snr_val = -20   # SNR Value to train using\"\"\"\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:4: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  after removing the cwd from sys.path.\n",
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part6.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part7.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part8.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part9.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part10.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part11.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part12.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part13.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part14.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part15.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part16.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part17.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part18.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part19.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part20.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part21.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part22.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part23.h5\n",
      "X-size： (1440000, 1024, 2)\n",
      "Y-size： (1440000, 24)\n"
     ]
    }
   ],
   "source": [
    "# In[2]:\n",
    "# Dataset setup\n",
    "\n",
    "f = h5py.File(r'C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part0.h5')\n",
    "sample_num = f['X'].shape[0]\n",
    "idx = np.random.choice(range(0,sample_num),size=60000)\n",
    "# idx = np.random.choice(range(0,sample_num),size=30000)\n",
    "X = f['X'][:][idx]\n",
    "Y = f['Y'][:][idx]\n",
    "\n",
    "f.close()\n",
    "\n",
    "for i in range(1,24):\n",
    "    if i%1 == 0:\n",
    "        !free -m\n",
    "    '''if i == 10:\n",
    "        continue'''\n",
    "    filename = r'C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part'+str(i) + '.h5'\n",
    "    print(filename)\n",
    "    f = h5py.File(filename,'r')\n",
    "    X = np.vstack((X,f['X'][:][idx]))\n",
    "    Y = np.vstack((Y,f['Y'][:][idx]))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "\n",
    "print('X-size：',X.shape)\n",
    "print('Y-size：',Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1152000, 1024, 2)\n",
      "Y_train: (1152000, 24)\n",
      "X_test: (288000, 1024, 2)\n",
      "Y_test: (288000, 24)\n",
      "Shape of X_train before PCA (1152000, 4)\n",
      "Shape of X_train after PCA (1152000, 4)\n",
      "Final shape of X_train (1152000, 2, 2)\n",
      "Final shape of X_test (288000, 2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'X_train = []\\nY_train = []\\nX_train_SNR_idx = []\\nX_train_SNR = map(lambda x: lbl[x][1], train_idx)\\nfor train_snr, train_index in zip(X_train_SNR, train_idx):\\n    if train_snr == snr_val:\\n        X_train_SNR_idx.append(train_index)\\nX_train = X[X_train_SNR_idx]\\nY_train = to_onehot(map(lambda x: mods.index(lbl[x][0]), X_train_SNR_idx))'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[3]:\n",
    "# Partition the dataset into training and testing datasets\n",
    "\n",
    "n_examples = X.shape[0]\n",
    "n_train = int(n_examples * 0.8)   \n",
    "train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)  #Randomly select training sample subscript\n",
    "test_idx = list(set(range(0,n_examples))-set(train_idx)) #Test sample index\n",
    "X_train = X[train_idx]  #training samples\n",
    "X_test =  X[test_idx]  #testing samples\n",
    "Y_train = Y[train_idx]\n",
    "Y_test = Y[test_idx]\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"Y_train:\",Y_train.shape)\n",
    "print(\"X_test:\",X_test.shape)\n",
    "print(\"Y_test:\",Y_test.shape)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Heuristic Sub Sampling\n",
    "\"\"\"n_samples = sub_samples\n",
    "new_X_train = list()\n",
    "for wave_idx, wave in enumerate(X_train):\n",
    "\tamp_list = [(iq_idx, ((iq_val[0] ** 2) + (iq_val[1] ** 2) ** 0.5)) for iq_idx, iq_val in enumerate(wave.transpose(1, 0))]\n",
    "\tamp_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\tamp_list = amp_list[:n_samples]\n",
    "\tamp_list.sort(key=lambda x: x[0])\n",
    "\tamp_list = [amp_val[0] for amp_val in amp_list]\n",
    "\twave = wave.transpose(1, 0)\n",
    "\twave = wave[amp_list]\n",
    "\twave = wave.transpose(1, 0)\n",
    "\tnew_X_train.append(wave)\n",
    "X_train = np.stack(new_X_train)\n",
    "\n",
    "new_X_test = list()\n",
    "for wave_idx, wave in enumerate(X_test):\n",
    "\tamp_list = [(iq_idx, ((iq_val[0] ** 2) + (iq_val[1] ** 2) ** 0.5)) for iq_idx, iq_val in enumerate(wave.transpose(1, 0))]\n",
    "\tamp_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\tamp_list = amp_list[:n_samples]\n",
    "\tamp_list.sort(key=lambda x: x[0])\n",
    "\tamp_list = [amp_val[0] for amp_val in amp_list]\n",
    "\twave = wave.transpose(1, 0)\n",
    "\twave = wave[amp_list]\n",
    "\twave = wave.transpose(1, 0)\n",
    "\tnew_X_test.append(wave)\n",
    "X_test = np.stack(new_X_test)\n",
    "\n",
    "print('Number of amplitudes after heuristic sub sampling:', X_train.shape)\"\"\"\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Random Sub Sampling\n",
    "\"\"\"n_samples = sub_samples\n",
    "sample_idx = np.random.choice(range(0,128), size=n_samples, replace=False)\n",
    "X_train = X_train.transpose((2, 1, 0))\n",
    "X_train = X_train[sample_idx]\n",
    "X_train = X_train.transpose((2, 1, 0))\n",
    "X_test = X_test.transpose((2, 1, 0))\n",
    "X_test = X_test[sample_idx]\n",
    "X_test = X_test.transpose((2, 1, 0))\"\"\"\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Uniform Sub Sampling\n",
    "'''n_samples = sub_samples\n",
    "sample_idx = [num for num in range(0, 128, 128//n_samples)]\n",
    "X_train = X_train.transpose((2, 1, 0))\n",
    "X_train = X_train[sample_idx]\n",
    "X_train = X_train.transpose((2, 1, 0))\n",
    "X_test = X_test.transpose((2, 1, 0))\n",
    "X_test = X_test[sample_idx]\n",
    "X_test = X_test.transpose((2, 1, 0))'''\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# PCA\n",
    "X_train = X_train.transpose((1, 0, 2))\n",
    "X_train = np.append(X_train[0], X_train[1], axis=1)\n",
    "pca_apply = pca.fit(X_train)\n",
    "print('Shape of X_train before PCA', np.shape(X_train))\n",
    "X_train = pca_apply.transform(X_train)\n",
    "print('Shape of X_train after PCA', np.shape(X_train))\n",
    "X_test = X_test.transpose((1, 0, 2))\n",
    "X_test = np.append(X_test[0], X_test[1], axis=1)\n",
    "X_test = pca_apply.transform(X_test)\n",
    "X_train = np.stack((X_train[:, :int(len(X_train[0])/2)], X_train[:, int(len(X_train[0])/2):]), axis=1)\n",
    "X_test = np.stack((X_test[:, :int(len(X_test[0])/2)], X_test[:, int(len(X_test[0])/2):]), axis=1)\n",
    "print('Final shape of X_train', np.shape(X_train))\n",
    "print('Final shape of X_test', np.shape(X_test))\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# SNR Training\n",
    "\"\"\"X_train = []\n",
    "Y_train = []\n",
    "X_train_SNR_idx = []\n",
    "X_train_SNR = map(lambda x: lbl[x][1], train_idx)\n",
    "for train_snr, train_index in zip(X_train_SNR, train_idx):\n",
    "    if train_snr == snr_val:\n",
    "        X_train_SNR_idx.append(train_index)\n",
    "X_train = X[X_train_SNR_idx]\n",
    "Y_train = to_onehot(map(lambda x: mods.index(lbl[x][0]), X_train_SNR_idx))\"\"\"\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In[4]:\n",
    "in_shp = list(X_train.shape[1:])\n",
    "\n",
    "classes = ['32PSK',\n",
    " '16APSK',\n",
    " '32QAM',\n",
    " 'FM',\n",
    " 'GMSK',\n",
    " '32APSK',\n",
    " 'OQPSK',\n",
    " '8ASK',\n",
    " 'BPSK',\n",
    " '8PSK',\n",
    " 'AM-SSB-SC',\n",
    " '4ASK',\n",
    " '16PSK',\n",
    " '64APSK',\n",
    " '128QAM',\n",
    " '128APSK',\n",
    " 'AM-DSB-SC',\n",
    " 'AM-SSB-WC',\n",
    " '64QAM',\n",
    " 'QPSK',\n",
    " '256QAM',\n",
    " 'AM-DSB-WC',\n",
    " 'OOK',\n",
    " '16QAM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_8 (Reshape)          (None, 1, 2, 2)           0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPaddi (None, 1, 2, 6)           0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 256, 2, 4)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256, 2, 4)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPaddi (None, 256, 2, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 256, 1, 6)         393472    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256, 1, 6)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPaddi (None, 256, 1, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 80, 1, 8)          61520     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 80, 1, 8)          0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_18 (ZeroPaddi (None, 80, 1, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 80, 1, 10)         19280     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 80, 1, 10)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_19 (ZeroPaddi (None, 80, 1, 14)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1120)              0         \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 1, 1120)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                234200    \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 256)               13056     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 24)                6168      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "reshape_10 (Reshape)         (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 728,720\n",
      "Trainable params: 728,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# In[5]:\n",
    "# Build the NN Model\n",
    "# Build VT-CNN2 Neural Net model using Keras primitives -- \n",
    "#  - Reshape [N,2,128] to [N,1,2,128] on input\n",
    "#  - Pass through 2 2DConv/ReLu layers\n",
    "#  - Pass through 2 Dense layers (ReLu and Softmax)\n",
    "#  - Perform categorical cross entropy optimization\n",
    "\n",
    "dr = 0.6 # dropout rate (%)\n",
    "model = Sequential()\n",
    "model.add(Reshape([1]+in_shp, input_shape=in_shp))\n",
    "model.add(ZeroPadding2D((0, 2), data_format=\"channels_first\"))\n",
    "model.add(Conv2D(kernel_initializer=\"glorot_uniform\", name=\"conv1\", activation=\"relu\", data_format=\"channels_first\", padding=\"valid\", filters=256, kernel_size=(1, 3)))\n",
    "model.add(Dropout(dr))\n",
    "\n",
    "model.add(ZeroPadding2D((0, 2), data_format=\"channels_first\"))\n",
    "model.add(Conv2D(kernel_initializer=\"glorot_uniform\", name=\"conv2\", activation=\"relu\", data_format=\"channels_first\", padding=\"valid\", filters=256, kernel_size=(2, 3)))\n",
    "model.add(Dropout(dr))\n",
    "\n",
    "model.add(ZeroPadding2D((0, 2), data_format=\"channels_first\"))\n",
    "model.add(Conv2D(kernel_initializer=\"glorot_uniform\", name=\"conv3\", activation=\"relu\", data_format=\"channels_first\", padding=\"valid\", filters=80, kernel_size=(1, 3)))\n",
    "model.add(Dropout(dr))\n",
    "\n",
    "model.add(ZeroPadding2D((0, 2), data_format=\"channels_first\"))\n",
    "model.add(Conv2D(kernel_initializer=\"glorot_uniform\", name=\"conv4\", activation=\"relu\", data_format=\"channels_first\", padding=\"valid\", filters=80, kernel_size=(1, 3)))\n",
    "model.add(Dropout(dr))\n",
    "model.add(ZeroPadding2D((0, 2), data_format=\"channels_first\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "# 128 (1) -> 11200; 64 (1/2) -> 6080; 32 (1/4) -> 3520; 16 (1/8) -> 2240; 8 (1/16) -> 1600; 4 (1/32) -> 1280\n",
    "model.add(Reshape((1,1120)))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(256, activation='relu', kernel_initializer='he_normal', name=\"dense1\"))\n",
    "model.add(Dropout(dr))\n",
    "model.add(Dense( len(classes), kernel_initializer='he_normal', name=\"dense2\" ))\n",
    "model.add(Activation('softmax'))\n",
    "model.add(Reshape([len(classes)]))\n",
    "#opt=adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[6]:\n",
    "# Set up some params \n",
    "nb_epoch = 500     # number of epochs to train on\n",
    "batch_size = 1024  # training batch size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.0139s). Check your callbacks.\n",
      "1125/1125 - 14s - loss: 2.9590 - accuracy: 0.0918 - val_loss: 2.9340 - val_accuracy: 0.1028\n",
      "Epoch 2/500\n",
      "1125/1125 - 12s - loss: 2.9037 - accuracy: 0.1084 - val_loss: 2.9349 - val_accuracy: 0.1090\n",
      "Epoch 3/500\n",
      "1125/1125 - 14s - loss: 2.8803 - accuracy: 0.1208 - val_loss: 2.8912 - val_accuracy: 0.1194\n",
      "Epoch 4/500\n",
      "1125/1125 - 14s - loss: 2.8633 - accuracy: 0.1263 - val_loss: 2.8754 - val_accuracy: 0.1223\n",
      "Epoch 5/500\n",
      "1125/1125 - 14s - loss: 2.8533 - accuracy: 0.1296 - val_loss: 2.8539 - val_accuracy: 0.1317\n",
      "Epoch 6/500\n",
      "1125/1125 - 14s - loss: 2.8465 - accuracy: 0.1315 - val_loss: 2.8506 - val_accuracy: 0.1260\n",
      "Epoch 7/500\n",
      "1125/1125 - 14s - loss: 2.8413 - accuracy: 0.1328 - val_loss: 2.8423 - val_accuracy: 0.1336\n",
      "Epoch 8/500\n",
      "1125/1125 - 13s - loss: 2.8375 - accuracy: 0.1346 - val_loss: 2.8490 - val_accuracy: 0.1309\n",
      "Epoch 9/500\n",
      "1125/1125 - 14s - loss: 2.8340 - accuracy: 0.1352 - val_loss: 2.8335 - val_accuracy: 0.1366\n",
      "Epoch 10/500\n",
      "1125/1125 - 14s - loss: 2.8302 - accuracy: 0.1364 - val_loss: 2.8285 - val_accuracy: 0.1356\n",
      "Epoch 11/500\n",
      "1125/1125 - 14s - loss: 2.8278 - accuracy: 0.1372 - val_loss: 2.8098 - val_accuracy: 0.1419\n",
      "Epoch 12/500\n",
      "1125/1125 - 12s - loss: 2.8256 - accuracy: 0.1378 - val_loss: 2.8211 - val_accuracy: 0.1359\n",
      "Epoch 13/500\n",
      "1125/1125 - 12s - loss: 2.8237 - accuracy: 0.1385 - val_loss: 2.8169 - val_accuracy: 0.1391\n",
      "Epoch 14/500\n",
      "1125/1125 - 12s - loss: 2.8221 - accuracy: 0.1386 - val_loss: 2.8189 - val_accuracy: 0.1376\n",
      "Epoch 15/500\n",
      "1125/1125 - 14s - loss: 2.8202 - accuracy: 0.1390 - val_loss: 2.8079 - val_accuracy: 0.1402\n",
      "Epoch 16/500\n",
      "1125/1125 - 14s - loss: 2.8188 - accuracy: 0.1393 - val_loss: 2.7919 - val_accuracy: 0.1445\n",
      "Epoch 17/500\n",
      "1125/1125 - 12s - loss: 2.8177 - accuracy: 0.1400 - val_loss: 2.8051 - val_accuracy: 0.1428\n",
      "Epoch 18/500\n",
      "1125/1125 - 12s - loss: 2.8162 - accuracy: 0.1404 - val_loss: 2.7958 - val_accuracy: 0.1432\n",
      "Epoch 19/500\n",
      "1125/1125 - 12s - loss: 2.8155 - accuracy: 0.1404 - val_loss: 2.7995 - val_accuracy: 0.1447\n",
      "Epoch 20/500\n",
      "1125/1125 - 14s - loss: 2.8146 - accuracy: 0.1404 - val_loss: 2.7862 - val_accuracy: 0.1455\n",
      "Epoch 21/500\n",
      "1125/1125 - 14s - loss: 2.8138 - accuracy: 0.1411 - val_loss: 2.7859 - val_accuracy: 0.1460\n",
      "Epoch 22/500\n",
      "1125/1125 - 12s - loss: 2.8130 - accuracy: 0.1416 - val_loss: 2.7888 - val_accuracy: 0.1449\n",
      "Epoch 23/500\n",
      "1125/1125 - 12s - loss: 2.8125 - accuracy: 0.1417 - val_loss: 2.7929 - val_accuracy: 0.1440\n",
      "Epoch 24/500\n",
      "1125/1125 - 14s - loss: 2.8116 - accuracy: 0.1417 - val_loss: 2.7827 - val_accuracy: 0.1463\n",
      "Epoch 25/500\n",
      "1125/1125 - 14s - loss: 2.8108 - accuracy: 0.1422 - val_loss: 2.7822 - val_accuracy: 0.1462\n",
      "Epoch 26/500\n",
      "1125/1125 - 12s - loss: 2.8104 - accuracy: 0.1421 - val_loss: 2.7870 - val_accuracy: 0.1457\n",
      "Epoch 27/500\n",
      "1125/1125 - 14s - loss: 2.8098 - accuracy: 0.1420 - val_loss: 2.7810 - val_accuracy: 0.1470\n",
      "Epoch 28/500\n",
      "1125/1125 - 12s - loss: 2.8093 - accuracy: 0.1427 - val_loss: 2.7816 - val_accuracy: 0.1462\n",
      "Epoch 29/500\n",
      "1125/1125 - 14s - loss: 2.8088 - accuracy: 0.1427 - val_loss: 2.7780 - val_accuracy: 0.1520\n",
      "Epoch 30/500\n",
      "1125/1125 - 12s - loss: 2.8082 - accuracy: 0.1424 - val_loss: 2.7788 - val_accuracy: 0.1493\n",
      "Epoch 31/500\n",
      "1125/1125 - 12s - loss: 2.8080 - accuracy: 0.1426 - val_loss: 2.7784 - val_accuracy: 0.1497\n",
      "Epoch 32/500\n",
      "1125/1125 - 14s - loss: 2.8079 - accuracy: 0.1428 - val_loss: 2.7740 - val_accuracy: 0.1499\n",
      "Epoch 33/500\n",
      "1125/1125 - 12s - loss: 2.8072 - accuracy: 0.1428 - val_loss: 2.7836 - val_accuracy: 0.1457\n",
      "Epoch 34/500\n",
      "1125/1125 - 12s - loss: 2.8069 - accuracy: 0.1426 - val_loss: 2.7770 - val_accuracy: 0.1491\n",
      "Epoch 35/500\n",
      "1125/1125 - 12s - loss: 2.8062 - accuracy: 0.1434 - val_loss: 2.7760 - val_accuracy: 0.1482\n",
      "Epoch 36/500\n",
      "1125/1125 - 12s - loss: 2.8057 - accuracy: 0.1431 - val_loss: 2.7781 - val_accuracy: 0.1482\n",
      "Epoch 37/500\n",
      "1125/1125 - 12s - loss: 2.8055 - accuracy: 0.1435 - val_loss: 2.7764 - val_accuracy: 0.1500\n",
      "Epoch 38/500\n",
      "1125/1125 - 12s - loss: 2.8061 - accuracy: 0.1434 - val_loss: 2.7743 - val_accuracy: 0.1487\n",
      "Epoch 39/500\n",
      "1125/1125 - 12s - loss: 2.8053 - accuracy: 0.1440 - val_loss: 2.7761 - val_accuracy: 0.1487\n",
      "Epoch 40/500\n",
      "1125/1125 - 14s - loss: 2.8050 - accuracy: 0.1437 - val_loss: 2.7717 - val_accuracy: 0.1511\n",
      "Epoch 41/500\n",
      "1125/1125 - 12s - loss: 2.8050 - accuracy: 0.1436 - val_loss: 2.7755 - val_accuracy: 0.1478\n",
      "Epoch 42/500\n",
      "1125/1125 - 13s - loss: 2.8049 - accuracy: 0.1436 - val_loss: 2.7776 - val_accuracy: 0.1487\n",
      "Epoch 43/500\n",
      "1125/1125 - 13s - loss: 2.8042 - accuracy: 0.1436 - val_loss: 2.7736 - val_accuracy: 0.1487\n",
      "Epoch 44/500\n",
      "1125/1125 - 12s - loss: 2.8044 - accuracy: 0.1435 - val_loss: 2.7724 - val_accuracy: 0.1478\n",
      "Epoch 45/500\n",
      "1125/1125 - 13s - loss: 2.8044 - accuracy: 0.1437 - val_loss: 2.7734 - val_accuracy: 0.1487\n",
      "Epoch 46/500\n",
      "1125/1125 - 14s - loss: 2.8036 - accuracy: 0.1437 - val_loss: 2.7696 - val_accuracy: 0.1514\n",
      "Epoch 47/500\n",
      "1125/1125 - 12s - loss: 2.8047 - accuracy: 0.1434 - val_loss: 2.7718 - val_accuracy: 0.1472\n",
      "Epoch 48/500\n",
      "1125/1125 - 14s - loss: 2.8033 - accuracy: 0.1440 - val_loss: 2.7672 - val_accuracy: 0.1507\n",
      "Epoch 49/500\n",
      "1125/1125 - 12s - loss: 2.8034 - accuracy: 0.1436 - val_loss: 2.7729 - val_accuracy: 0.1489\n",
      "Epoch 50/500\n",
      "1125/1125 - 12s - loss: 2.8033 - accuracy: 0.1438 - val_loss: 2.7724 - val_accuracy: 0.1500\n",
      "Epoch 51/500\n",
      "1125/1125 - 12s - loss: 2.8041 - accuracy: 0.1433 - val_loss: 2.7752 - val_accuracy: 0.1512\n",
      "Epoch 52/500\n",
      "1125/1125 - 12s - loss: 2.8036 - accuracy: 0.1436 - val_loss: 2.7764 - val_accuracy: 0.1476\n",
      "Epoch 53/500\n",
      "1125/1125 - 13s - loss: 2.8033 - accuracy: 0.1435 - val_loss: 2.7679 - val_accuracy: 0.1490\n",
      "Epoch 54/500\n",
      "1125/1125 - 13s - loss: 2.8027 - accuracy: 0.1435 - val_loss: 2.7734 - val_accuracy: 0.1490\n",
      "Epoch 55/500\n",
      "1125/1125 - 12s - loss: 2.8027 - accuracy: 0.1439 - val_loss: 2.7705 - val_accuracy: 0.1518\n",
      "Epoch 56/500\n",
      "1125/1125 - 14s - loss: 2.8024 - accuracy: 0.1437 - val_loss: 2.7661 - val_accuracy: 0.1506\n",
      "Epoch 57/500\n",
      "1125/1125 - 13s - loss: 2.8029 - accuracy: 0.1438 - val_loss: 2.7719 - val_accuracy: 0.1470\n",
      "Epoch 58/500\n",
      "1125/1125 - 12s - loss: 2.8033 - accuracy: 0.1440 - val_loss: 2.7760 - val_accuracy: 0.1473\n",
      "Epoch 59/500\n",
      "1125/1125 - 13s - loss: 2.8027 - accuracy: 0.1441 - val_loss: 2.7699 - val_accuracy: 0.1524\n",
      "Epoch 60/500\n",
      "1125/1125 - 13s - loss: 2.8027 - accuracy: 0.1439 - val_loss: 2.7697 - val_accuracy: 0.1490\n",
      "Epoch 61/500\n",
      "1125/1125 - 13s - loss: 2.8023 - accuracy: 0.1438 - val_loss: 2.7668 - val_accuracy: 0.1488\n",
      "Epoch 62/500\n",
      "1125/1125 - 13s - loss: 2.8024 - accuracy: 0.1437 - val_loss: 2.7727 - val_accuracy: 0.1494\n",
      "Epoch 63/500\n",
      "1125/1125 - 14s - loss: 2.8016 - accuracy: 0.1437 - val_loss: 2.7650 - val_accuracy: 0.1534\n",
      "Epoch 64/500\n",
      "1125/1125 - 14s - loss: 2.8019 - accuracy: 0.1436 - val_loss: 2.7685 - val_accuracy: 0.1473\n",
      "Epoch 65/500\n",
      "1125/1125 - 13s - loss: 2.8023 - accuracy: 0.1437 - val_loss: 2.7670 - val_accuracy: 0.1504\n",
      "Epoch 66/500\n",
      "1125/1125 - 12s - loss: 2.8022 - accuracy: 0.1441 - val_loss: 2.7689 - val_accuracy: 0.1503\n",
      "Epoch 67/500\n",
      "1125/1125 - 13s - loss: 2.8019 - accuracy: 0.1436 - val_loss: 2.7681 - val_accuracy: 0.1491\n",
      "Epoch 68/500\n",
      "1125/1125 - 13s - loss: 2.8026 - accuracy: 0.1435 - val_loss: 2.7662 - val_accuracy: 0.1498\n",
      "Epoch 69/500\n",
      "1125/1125 - 12s - loss: 2.8024 - accuracy: 0.1436 - val_loss: 2.7700 - val_accuracy: 0.1507\n",
      "Epoch 70/500\n",
      "1125/1125 - 13s - loss: 2.8022 - accuracy: 0.1438 - val_loss: 2.7757 - val_accuracy: 0.1466\n",
      "Epoch 71/500\n",
      "1125/1125 - 13s - loss: 2.8018 - accuracy: 0.1435 - val_loss: 2.7696 - val_accuracy: 0.1486\n",
      "Epoch 72/500\n",
      "1125/1125 - 13s - loss: 2.8017 - accuracy: 0.1439 - val_loss: 2.7666 - val_accuracy: 0.1498\n",
      "Epoch 73/500\n",
      "1125/1125 - 13s - loss: 2.8019 - accuracy: 0.1440 - val_loss: 2.7691 - val_accuracy: 0.1507\n",
      "Epoch 74/500\n",
      "1125/1125 - 13s - loss: 2.8019 - accuracy: 0.1438 - val_loss: 2.7707 - val_accuracy: 0.1488\n",
      "Epoch 75/500\n",
      "1125/1125 - 13s - loss: 2.8019 - accuracy: 0.1435 - val_loss: 2.7683 - val_accuracy: 0.1501\n",
      "Epoch 76/500\n",
      "1125/1125 - 13s - loss: 2.8015 - accuracy: 0.1440 - val_loss: 2.7685 - val_accuracy: 0.1466\n",
      "Epoch 77/500\n",
      "1125/1125 - 12s - loss: 2.8017 - accuracy: 0.1435 - val_loss: 2.7661 - val_accuracy: 0.1496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "1125/1125 - 13s - loss: 2.8017 - accuracy: 0.1437 - val_loss: 2.7659 - val_accuracy: 0.1498\n"
     ]
    }
   ],
   "source": [
    "# In[7]:\n",
    "# Train the Model\n",
    "# perform training ...\n",
    "#   - call the main training loop in keras for our network+dataset\n",
    "filepath = 'convmodrecnets_CLDNN_multigpu_0.6.wts.h5'\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train,\n",
    "    Y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=nb_epoch,\n",
    "    verbose=2,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='auto')\n",
    "    ])\n",
    "# we re-load the best weights once training is finished\n",
    "model.load_weights(filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[8]:\n",
    "# Evaluate and Plot Model Performance\n",
    "# Show simple version of performance\n",
    "score = model.evaluate(X_test, Y_test, verbose=0, batch_size=batch_size)\n",
    "print score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "# Show loss curves \n",
    "plt.figure()\n",
    "plt.title('Training performance')\n",
    "plt.plot(history.epoch, history.history['loss'], label='train loss+error')\n",
    "plt.plot(history.epoch, history.history['val_loss'], label='val_error')\n",
    "plt.legend()\n",
    "plt.savefig('Train_perf.png', dpi=100)\t#save image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[10]:\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[11]:\n",
    "# Plot confusion matrix\n",
    "test_Y_hat = model.predict(X_test, batch_size=batch_size)\n",
    "conf = np.zeros([len(classes),len(classes)])\n",
    "confnorm = np.zeros([len(classes),len(classes)])\n",
    "for i in range(0,X_test.shape[0]):\n",
    "    j = list(Y_test[i,:]).index(1)\n",
    "    k = int(np.argmax(test_Y_hat[i,:]))\n",
    "    conf[j,k] = conf[j,k] + 1\n",
    "for i in range(0,len(classes)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_confusion_matrix(confnorm, labels=classes)\n",
    "\n",
    "# In[12]:\n",
    "# Plot confusion matrix\n",
    "acc = {}\n",
    "for snr in snrs:\n",
    "    # extract classes @ SNR\n",
    "    test_SNRs = map(lambda x: lbl[x][1], test_idx)\n",
    "    test_X_i = X_test[np.where(np.array(test_SNRs)==snr)]\n",
    "    test_Y_i = Y_test[np.where(np.array(test_SNRs)==snr)]    \n",
    "\n",
    "    # estimate classes\n",
    "    test_Y_i_hat = model.predict(test_X_i)\n",
    "    conf = np.zeros([len(classes),len(classes)])\n",
    "    confnorm = np.zeros([len(classes),len(classes)])\n",
    "    for i in range(0,test_X_i.shape[0]):\n",
    "        j = list(test_Y_i[i,:]).index(1)\n",
    "        k = int(np.argmax(test_Y_i_hat[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(classes)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confnorm, labels=classes, title=\"CLDNN Confusion Matrix (SNR=%d)\"%(snr))\n",
    "    #figname = \"./CLDNN_result/real_cldnn/CLDNN-confusion-matrix\" + str(snr)+\".png\"\n",
    "    #plt.savefig(figname)     \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    print \"Overall Accuracy: \", cor / (cor+ncor)\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)\n",
    "\n",
    "# In[13]:\n",
    "# Save results to a pickle file for plotting later\n",
    "print acc\n",
    "fd = open('results_cldnn_d0.5.dat','wb')\n",
    "cPickle.dump( (\"CLDNN\", 0.5, acc) , fd )\n",
    "\n",
    "# In[14]:\n",
    "# Plot accuracy curve\n",
    "plt.plot(snrs, map(lambda x: acc[x], snrs))\n",
    "plt.xlabel(\"Signal to Noise Ratio\")\n",
    "plt.ylabel(\"Classification Accuracy\")\n",
    "plt.title(\"CLDNN Classification Accuracy on RadioML 2016.10 Alpha\")\n",
    "#plt.savefig('./CLDNN_result/real_cldnn/CLDNN-accuracy.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
